# Тестирование и Сравнительный Анализ
Для оценки эффективности реализованного модуля `KnowledgeBase`, в частности его компонента `GraphExtractor`, было проведено сравнительное тестирование с существующей системой MiniRAG. Тестирование проводилось на англоязычном текстовом документе объемом около 6 000 токенов с использованием различных конфигураций моделей Gemma.

**Основные характеристики систем:**

| Критерий             | MiniRAG                                                | StoryMindAI (реализованный модуль)                       |
|----------------------|--------------------------------------------------------|----------------------------------------------------------|
| **Преимущества** | - Классификация узлов<br>- Многоязычность              | - Меньший размер запросов к LLM (~500 токенов)<br>- Большее количество генерируемых связей и связанных узлов<br>- Экономия видеопамяти (за счет меньших запросов)<br>- Эффективность при малых размерах чанков текста |
| **Недостатки** | - Меньше связей (0 связей при 1B моделях)<br>- Высокая стоимость токенов при малых чанках | - Поддерживает только английский язык (на данный момент) |

**Сравнение результатов генерации графа по моделям:**

* **Gemma-3-4b-it с восьмибитным квантование (4.64 GB)**
    | Система        | Узлы | Связи | Связанные узлы |
    |----------------|------|-------|----------------|
    | MiniRAG        | 179  | 101   | 111            |
    | StoryMindAI    | 138  | 147   | 127            |

* **Gemma-3-4b-it с четырехбитным квантоваием q4_k_m (3.11 GB)**
    | Система        | Узлы | Связи | Связанные узлы |
    |----------------|------|-------|----------------|
    | MiniRAG        | 130  | 103   | 98             |
    | StoryMindAI    | 126  | 133   | 105            |

* **Gemma-3-1B-it с четырехбитным квантоваием qat-q4_0 (0.72 GB)**
    | Система        | Узлы | Связи | Связанные узлы | Примечания                                                                                                         |
    |----------------|------|-------|----------------|--------------------------------------------------------------------------------------------------------------------|
    | MiniRAG        | 34   | 0     | 0              | Чанк 1000 токенов                                                                                                  |
    | MiniRAG        | 77   | 32    | 37             | Чанк 256 токенов (37 чанков, ~3000 токенов на ответ => ~111k токенов суммарно для генерации)                         |
    | StoryMindAI    | 106  | 96    | 76             | Стандартный режим работы, чанк ~1000 символов (~250 токенов)                                                                         |

**Анализ использования токенов и вычислительных ресурсов:**

| Система        | Количество запросов к LLM | Входные токены (прибл.) | Выходные токены (прибл.) | Всего токенов (прибл.) |
|----------------|---------------------------|--------------------------|-------------------------|------------------------|
| MiniRAG        | 7                         | ~21 000                  | ~7 000                  | ~28 000                |
| StoryMindAI    | 29                        | ~14 500                  | ~13 500                 | ~28 000                |

**Ключевой вывод по ресурсам:** 
Обе системы использовали примерно одинаковое общее количество токенов (~28k) для обработки документа. Однако StoryMindAI достигает этого за счет большего числа более коротких запросов, что снижает требования к видеопамяти для каждого отдельного вызова LLM и уменьшает объем входных токенов по сравнению с MiniRAG.

**Аналитика и выводы по тестированию:**

* **Преимущества StoryMindAI:**
    * *Эффективность использования ресурсов:* Требует значительно меньше входных токенов на один запрос к LLM (примерно на 80% меньше, чем MiniRAG в данном тесте), что способствует экономии видеопамяти. Генерирует больше связей между сущностями, особенно при использовании моделей очень малого размера (например, 96 связей против 32 у MiniRAG на модели Gemma-3-1B-it).

* **Преимущества MiniRAG:**
    * *Функциональная гибкость:* Обладает встроенной поддержкой классификации узлов графа и многоязычности, что не было реализовано в текущей версии StoryMindAI.

Результаты тестирования подтверждают эффективность разработанного подхода к извлечению знаний и построению графа, особенно для работы с ограниченными вычислительными ресурсами и небольшими LLM.
